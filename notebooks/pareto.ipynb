{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains:\n",
    "\n",
    "* Quick pipeline similar to main.py\n",
    "* Pareto 3D with blob size etc\n",
    "* New corridor computation --> fix max number of edges\n",
    "* KSP with similarity metrics\n",
    "* KSP with overlap (new formulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# utils imports\n",
    "from power_planner.data_reader import DataReader\n",
    "from power_planner import graphs\n",
    "from power_planner.plotting import plot_path_costs, plot_pipeline_paths, plot_path, plot_k_sp\n",
    "from power_planner.utils.utils import get_distance_surface, time_test_csv, compute_pylon_dists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FILES = \"../data\"\n",
    "\n",
    "# DEFINE CONFIGURATION\n",
    "ID = \"w_ksp_5\"  # str(round(time.time() / 60))[-5:]\n",
    "\n",
    "OUT_PATH = \"outputs/path_\" + ID\n",
    "SCALE_PARAM = 5  # args.scale\n",
    "# normal graph pipeline\n",
    "# PIPELINE = [(2, 50), (1, 0)]  # [(1, 0)]  # [(4, 80), (2, 50), (1, 0)]  #\n",
    "# random graph pipeline\n",
    "PIPELINE = [(1, 0)]  # [(0.9, 40), (0, 0)]\n",
    "\n",
    "GRAPH_TYPE = graphs.WeightedKSP\n",
    "# LineGraph, WeightedGraph, RandomWeightedGraph, RandomLineGraph, PowerBF\n",
    "# TwoPowerBF, WeightedKSP\n",
    "print(\"graph type:\", GRAPH_TYPE)\n",
    "# summarize: mean/max/min, remove: all/surrounding, sample: simple/watershed\n",
    "NOTES = \"None\"  # \"mean-all-simple\"\n",
    "\n",
    "IOPATH = os.path.join(PATH_FILES, \"data_dump_\" + str(SCALE_PARAM) + \".dat\")\n",
    "\n",
    "with open(\"../config.json\", \"r\") as infile:\n",
    "    cfg_dict = json.load(infile)  # Config(SCALE_PARAM)\n",
    "    cfg = SimpleNamespace(**cfg_dict)\n",
    "    cfg.PYLON_DIST_MIN, cfg.PYLON_DIST_MAX = compute_pylon_dists(\n",
    "        cfg.PYLON_DIST_MIN, cfg.PYLON_DIST_MAX, cfg.RASTER, SCALE_PARAM\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "with open(IOPATH, \"rb\") as infile:\n",
    "    data = pickle.load(infile)\n",
    "    (instance, instance_corr, start_inds, dest_inds) = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = GRAPH_TYPE(\n",
    "    instance, instance_corr, graphtool=cfg.GTNX, verbose=cfg.VERBOSE\n",
    ")\n",
    "\n",
    "graph.set_edge_costs(\n",
    "    data.layer_classes, data.class_weights, angle_weight=cfg.ANGLE_WEIGHT\n",
    ")\n",
    "graph.set_shift(\n",
    "    cfg.PYLON_DIST_MIN,\n",
    "    cfg.PYLON_DIST_MAX,\n",
    "    dest_inds - start_inds,\n",
    "    cfg.MAX_ANGLE,\n",
    "    max_angle_lg=cfg.MAX_ANGLE_LG\n",
    ")\n",
    "# add vertices\n",
    "graph.add_nodes()\n",
    "\n",
    "# START PIPELINE\n",
    "tic = time.time()\n",
    "corridor = np.ones(instance_corr.shape) * 0.5  # start with all\n",
    "output_paths = []\n",
    "plot_surfaces = []\n",
    "time_infos = []\n",
    "\n",
    "for (factor, dist) in PIPELINE:\n",
    "    print(\"----------- PIPELINE\", factor, dist, \"---------------\")\n",
    "    graph.set_corridor(corridor, start_inds, dest_inds, factor_or_n_edges=factor)\n",
    "    print(\"1) set cost rest\")\n",
    "    graph.add_edges()\n",
    "    print(\"2) added edges\", graph.n_edges)\n",
    "    print(\"number of vertices:\", graph.n_nodes)\n",
    "\n",
    "    # weighted sum of all costs\n",
    "    graph.sum_costs()\n",
    "    source_v, target_v = graph.add_start_and_dest(start_inds, dest_inds)\n",
    "    print(\"3) summed cost, get source and dest\")\n",
    "    # get actual best path\n",
    "    path, path_costs, cost_sum = graph.get_shortest_path(source_v, target_v)\n",
    "    print(\"4) shortest path\")\n",
    "    # save for inspection\n",
    "    output_paths.append((path, path_costs))\n",
    "    plot_surfaces.append(graph.cost_rest[2].copy())  # TODO: mean makes black\n",
    "    # get several paths --> here: pareto paths\n",
    "    paths = [path]\n",
    "    # graph.get_pareto(\n",
    "    #     np.arange(0, 1.1, 0.1), source_v, target_v, compare=[2, 3]\n",
    "    # )\n",
    "\n",
    "    time_infos.append(graph.time_logs.copy())\n",
    "\n",
    "    if cfg.VERBOSE:\n",
    "        del graph.time_logs['edge_list_times']\n",
    "        del graph.time_logs['add_edges_times']\n",
    "        print(graph.time_logs)\n",
    "\n",
    "    if dist > 0:\n",
    "        # PRINT AND SAVE timing test\n",
    "        time_test_csv(\n",
    "            ID, cfg.CSV_TIMES, SCALE_PARAM, cfg.GTNX, GRAPH_TYPE, graph,\n",
    "            path_costs, cost_sum, dist, 0, NOTES\n",
    "        )\n",
    "        # do specified numer of dilations\n",
    "        corridor = get_distance_surface(\n",
    "            graph.pos2node.shape, paths, mode=\"dilation\", n_dilate=dist\n",
    "        )\n",
    "        print(\"5) compute distance surface\")\n",
    "        # remove the edges of vertices in the corridor (to overwrite)\n",
    "        graph.remove_vertices(corridor, delete_padding=cfg.PYLON_DIST_MAX)\n",
    "        print(\"6) remove edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_costs(\n",
    "    instance * instance_corr,\n",
    "    path,\n",
    "    path_costs,\n",
    "    data.layer_classes,\n",
    "    buffer=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve pareto computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "\n",
    "We say that an alternative A dominates B if A outscores B regardless of the tradeoff between value and cost â€” that is, if A is both better and cheaper than B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "def plot_pareto_scatter_matrix(pareto, weights, classes):\n",
    "    df = pd.DataFrame(pareto)\n",
    "    sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_pareto_3d(pareto, weights, classes):\n",
    "    \"\"\"\n",
    "    3D plot of pareto points from 3 cost classes\n",
    "    Arguments:\n",
    "        pareto: np array of shape num_paths x 3, with costs for each path\n",
    "        weights: array or list of shape num_paths x 3, giving the weights that yielded the pareto costs\n",
    "        classes: list of 3 strings, the compared graphs\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = Axes3D(fig)\n",
    "    \n",
    "    for i in range(len(pareto)):\n",
    "        x, y, z = tuple(pareto[i])\n",
    "        col_weights = (np.asarray([weights[i]])+0.4)/1.4\n",
    "        # print(x,y,z, weights[i])\n",
    "        ax.scatter(x, y, z, marker='o', s=100, c=col_weights, label=weights[i])\n",
    "\n",
    "    ax.set_xlabel(classes[0], fontsize=15)\n",
    "    ax.set_ylabel(classes[1], fontsize=15)\n",
    "    ax.set_zlabel(classes[2], fontsize=15)\n",
    "    # manually define legend\n",
    "    legend_elements = [Line2D([0], [0],marker='o', color=[1,0,0], markersize=10, lw=0.1, label='only '+classes[0]),\n",
    "                       Line2D([0], [0], marker='o', color=[0,1,0],lw=0.1, label='only '+classes[1], markersize=10),\n",
    "                       Line2D([0], [0], marker='o', color=[0,0,1],lw=0.1, label='only '+classes[2], markersize=10)]\n",
    "\n",
    "    # Create the figure\n",
    "    ax.legend(handles=legend_elements, loc='upper center', fontsize=17)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def plot_pareto_scatter_3d(pareto, weights, classes, cost_sum=None):\n",
    "    \"\"\"\n",
    "    3D plot of pareto points from 3 cost classes\n",
    "    Arguments:\n",
    "        pareto: np array of shape num_paths x 3, with costs for each path\n",
    "        weights: array or list of shape num_paths x 3, giving the weights that yielded the pareto costs\n",
    "        classes: list of 3 strings, the compared graphs\n",
    "    \"\"\"\n",
    "    if cost_sum is not None:\n",
    "        norm_cost_sums = np.array(cost_sum)\n",
    "        norm_cost_sums = (norm_cost_sums-np.min(norm_cost_sums))/(np.max(norm_cost_sums)-np.min(norm_cost_sums))\n",
    "    print(norm_cost_sums)\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,5))\n",
    "    for i in range(3):\n",
    "        ax = fig.add_subplot(1,3,i+1)\n",
    "        ind1 = i\n",
    "        ind2 = (i+1)%3\n",
    "        for j in range(len(pareto)):\n",
    "            label=np.argmax(weights[j])\n",
    "            if cost_sum is not None:\n",
    "                size = norm_cost_sums[j] * 1000 + 20\n",
    "            else:\n",
    "                size = 100\n",
    "            col_weights = (np.asarray([weights[j]])+0.4)/1.4\n",
    "            ax.scatter(pareto[j,ind1], pareto[j,ind2], c=col_weights, s=size, label=label)\n",
    "        plt.xlabel(classes[ind1], fontsize=17)\n",
    "        plt.ylabel(classes[ind2], fontsize=17)\n",
    "    legend_elements = [Line2D([0], [0],marker='o', color=[1,0,0], markersize=10, lw=0.1, label='only '+classes[0]),\n",
    "                   Line2D([0], [0], marker='o', color=[0,1,0],lw=0.1, label='only '+classes[1], markersize=10),\n",
    "                     Line2D([0], [0], marker='o', color=[0,0,1],lw=0.1, label='only '+classes[2], markersize=10)]\n",
    "\n",
    "    # Create the figure\n",
    "    ax.legend(handles=legend_elements, loc='upper center', fontsize=17)\n",
    "    plt.savefig(\"pareto3d.pdf\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_pareto_scatter_2d(pareto, weights, classes, cost_sum=None):\n",
    "    \"\"\"\n",
    "    Scatter to compare two cost classes\n",
    "    Arguments:\n",
    "        pareto: np array of shape num_paths x 2, with costs for each path\n",
    "        weights: array or list of shape num_paths x 2, giving the weights that yielded the pareto costs\n",
    "        classes: list of 2 strings, the compared graphs\n",
    "    \"\"\"\n",
    "    if cost_sum is not None:\n",
    "        norm_cost_sums = np.array(cost_sum)\n",
    "        # normalize\n",
    "        norm_cost_sums = (norm_cost_sums-np.min(norm_cost_sums))/(np.max(norm_cost_sums)-np.min(norm_cost_sums))\n",
    "        size = norm_cost_sums * 1000 + 20\n",
    "    else:\n",
    "        size= 100\n",
    "    color = np.array([[w[0], 0, w[1]] for w in weights])\n",
    "    # scatter pareto curve\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(pareto[:,0], pareto[:,1], c= color, s=size)# color)\n",
    "    plt.gray()\n",
    "    plt.xlabel(classes[0], fontsize=15)\n",
    "    plt.ylabel(classes[1], fontsize=15)\n",
    "    # manually create legend\n",
    "    legend_elements = [Line2D([0], [0],marker='o', color=[1,0,0], markersize=10, lw=0.1, label='only '+classes[0]),\n",
    "                   Line2D([0], [0], marker='o', color=[0,0,1],lw=0.1, label='only '+classes[1], markersize=10)]\n",
    "    # Create the figure\n",
    "    plt.legend(handles=legend_elements, loc='upper center', fontsize=17)\n",
    "    plt.title(\"Pareto frontier for \" + classes[0] + \" vs \" + classes[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pareto(self, vary, source, dest, out_path=None, non_compare_weight=0, compare=[0, 1],plotting=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        vary: how many weights to explore, e.g 3 --> each cost class can have weight 0, 0.5 or 1\n",
    "        source, dest: as always the source and destination vertex\n",
    "        out_path: where to save the pareto figure(s)\n",
    "        compare: indices of cost classes to compare\n",
    "    Returns:\n",
    "        paths: All found paths\n",
    "        pareto: The costs for each combination of weights\n",
    "    \"\"\"\n",
    "    # initialize lists\n",
    "    pareto = list()\n",
    "    paths = list()\n",
    "    cost_sum = list()\n",
    "    # \n",
    "    cost_arrs = [cost.get_array() for cost in self.cost_props]\n",
    "    # [self.cost_props[comp].get_array() for comp in compare]\n",
    "    var_weights = np.around(np.linspace(0,1, vary),2)\n",
    "    \n",
    "    if len(compare)==2:\n",
    "        weights = [[v, 1-v] for v in var_weights]\n",
    "    elif len(compare)==3:\n",
    "        weights = list()\n",
    "        for w0 in var_weights:\n",
    "            for w1 in var_weights[var_weights<=1-w0]:\n",
    "                weights.append([w0, w1, 1-w0-w1])\n",
    "    else:\n",
    "        raise ValueError(\"argument compare can only have length 2 or 3\")\n",
    "    \n",
    "    best_sum = []\n",
    "    w_avail = np.sum(np.asarray(self.cost_weights)[compare])\n",
    "    print(w_avail)\n",
    "    for j in range(len(weights)):\n",
    "        w = self.cost_weights.copy() # np.zeros(len(cost_arrs)) + non_compare_weight\n",
    "        w[compare] = np.array(weights[j])*w_avail # replace these ones\n",
    "        # print(w)\n",
    "        self.weight.a = np.sum([cost_arrs[i] * w[i] for i in range(len(cost_arrs))], axis=0)\n",
    "        # get shortest path\n",
    "        path, path_costs, _ = self.get_shortest_path(source, dest)\n",
    "        pareto.append(np.sum(path_costs, axis=0)[compare])\n",
    "        print(np.sum(path_costs, axis=0))\n",
    "        print(w)\n",
    "        print(np.sum(path_costs))\n",
    "        print(\"--------\")\n",
    "        paths.append(path)\n",
    "        cost_sum.append(np.sum(path_costs))\n",
    "\n",
    "    # print best weighting\n",
    "    best_weight = np.argmin(cost_sum)\n",
    "    w = self.cost_weights.copy()\n",
    "    w[compare] = np.array(weights[best_weight]) * w_avail\n",
    "    print(\"Best weights:\", w, \"with (unweighted) costs:\", np.min(cost_sum))\n",
    "            \n",
    "    pareto = np.array(pareto)\n",
    "    classes = [self.cost_classes[comp] for comp in compare]\n",
    "    # Plotting\n",
    "    if plotting:\n",
    "        if len(compare)==2:\n",
    "            plot_pareto_scatter_2d(pareto, weights, classes, cost_sum=cost_sum)\n",
    "        elif len(compare)==3:\n",
    "            # plot_pareto_3d(pareto, weights, classes)\n",
    "            plot_pareto_scatter_3d(pareto, weights, classes, cost_sum=cost_sum)\n",
    "        # TODO: return weights as well\n",
    "    return paths, weights, cost_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pareto_out = get_pareto(\n",
    "    graph, 10, source_v, target_v, compare=[0, 2, 3], non_compare_weight=0.5, out_path=1, plotting=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_out[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_paths(pareto_out, inst, out_path=None):\n",
    "    \"\"\"\n",
    "    Plot k shortest paths on the instance\n",
    "    Arguments:\n",
    "        pareto_out: tuple of pareto outputs: (paths, weights, cost sums)\n",
    "        inst: instance to plot on\n",
    "    \"\"\"\n",
    "    # get relevant information\n",
    "    paths, weight_colours, cost_sum = pareto_out \n",
    "\n",
    "    # plot main image (cost surface)\n",
    "    plt.figure(figsize=(10, 20))\n",
    "    plt.imshow(np.swapaxes(inst, 1, 0))\n",
    "    # iterate over k shortest paths\n",
    "    for i, path in enumerate(paths):\n",
    "        path = np.asarray(path)\n",
    "        col = weight_colours[i]\n",
    "        if len(col)==2:\n",
    "            col = [col[0],0,col[1]]\n",
    "        plt.plot(\n",
    "            path[:, 0], path[:, 1], label=str(round(cost_sum[i], 2)), c= col, linewidth=3\n",
    "        )\n",
    "    # plot and save\n",
    "    leg = plt.legend(fontsize=15)\n",
    "    leg.set_title('Weighted costs', prop={'size': 15})\n",
    "    plt.axis(\"off\")\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path + \"_pareto_paths.pdf\")\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_pareto_paths(pareto_out, graph.instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_paths = get_pareto_3d(\n",
    "    graph, 3, source_v, target_v, compare=[0, 2, 3], non_compare_weight=0.5, out_path=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "paths = get_pareto(\n",
    "    graph, 10, source_v, target_v, compare=[0, 2, 3], out_path=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretaion: \n",
    "* possible to optimize for cultural and technical costs simultanously\n",
    "* if planning and cultural minimized, then technical tends to be high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random corridor new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_planner.utils.utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped = (np.unique(corridor_dilate).tolist())\n",
    "flipped.reverse()\n",
    "onedim = normalize(np.array(np.unique(corridor_dilate).tolist() + flipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = lambda x: np.exp(-(x-1)**2/(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.log(onedim))\n",
    "plt.plot(np.sqrt(onedim), label=\"sqrt\")\n",
    "plt.plot(onedim**2, label=\"squared\")\n",
    "plt.plot(normalize(onedim), label=\"lin\")\n",
    "plt.plot(gauss(onedim), label=\"gauss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "corr so small because negative values! --> half of the corridor just vanishes\n",
    "--> would need to take values close to 0.5 instead -> still using half of the points\n",
    "\n",
    "and actually not even 0.5, but such that cutoff corridor is at 0.5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = np.random.rand(1000)*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((arr1>arr2).astype(bool))/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to scale mean of corridor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New method (not cutting off corridor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corridor_dilate = get_distance_surface(\n",
    "            instance_corr.shape, [path], mode=\"dilation\", n_dilate=30\n",
    "        )\n",
    "factor = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corridor = corridor_dilate.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"gauss\"\n",
    "if mode == \"gauss\":\n",
    "    gauss = lambda x: np.exp(-(x - 1)**2 / (4))\n",
    "    corridor = normalize(gauss(normalize(corridor)))\n",
    "elif mode == \"same\":\n",
    "    corridor = normalize(corridor)\n",
    "elif mode == \"squared\":\n",
    "    corridor = normalize(corridor)**2\n",
    "elif mode == \"squareroot\":\n",
    "    corridor = np.sqrt(normalize(corridor))\n",
    "else:\n",
    "    raise NotImplementedError(\"mode must be gauss, squared...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(corridor)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val_now = np.mean(corridor[corridor>0])\n",
    "n_entris = len(corridor_dilate[corridor_dilate>0])\n",
    "print(\"entries orig\", len(corridor_dilate[corridor_dilate>0]), \"entries now\", len(corridor[corridor>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mean_val_now > 1-factor:\n",
    "    scale_factor = (1-factor)/mean_val_now\n",
    "    print(\"scale_factor\", scale_factor)\n",
    "    corridor = corridor*scale_factor\n",
    "else:\n",
    "    scale_factor = factor/(1-mean_val_now)\n",
    "    print(\"scale_factor\", scale_factor)\n",
    "    larger_zero = (corridor>0).astype(int)\n",
    "    corridor = 1-scale_factor + corridor*scale_factor\n",
    "    # have to reset to 0\n",
    "    corridor = corridor*larger_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(corridor[corridor>0]), \"should be\", (1-factor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.rand(*corridor.shape)\n",
    "corr_thresh = (corridor>arr).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "plt.imshow(corr_thresh)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.random.rand(*corridor.shape)\n",
    "corr_thresh = (corridor>arr).astype(int)\n",
    "leftover=len(corr_thresh[corr_thresh>0])\n",
    "print(\"entries leftover\", leftover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RATIO DEL\",(n_entris-leftover)/ n_entris,\"should be\", factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_entris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15000000 #edges actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "250 * 550 * 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get entries of hard_constraints\n",
    "# n_nodes_hard = len(self.hard_constraints[self.hard_constraints > 0])\n",
    "\n",
    "n_edges_desired = 5000000\n",
    "n_nodes_hard = len(corridor[corridor > 0])\n",
    "# edges_approx is the number of edges we would get if we take the whole corridor\n",
    "n_edges_approx = len(self.shifts) * n_nodes_hard\n",
    "print(\"desired\", n_edges_desired, \"n nodes in corridor\", n_nodes_hard, \"approximate_edges\", n_edges_approx)\n",
    "\n",
    "# ratio of edges to delete x\n",
    "ratio_keep = (n_edges_desired/n_edges_approx)\n",
    "# in the beginning: second part is 1, so simply taking the edge ratio\n",
    "# later: second part is smaller, so deleting less edges\n",
    "# min because might happen that corridor has less edges anyways then the desired maximum\n",
    "factor = min([1-ratio_keep, 1])\n",
    "print(ratio_keep, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.permutation(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve ksp - similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool.all import shortest_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(s1, s2, mode=\"IoU\"):\n",
    "    path_inter = len(s1.intersection(s2))\n",
    "    if mode==\"IoU\":\n",
    "        return path_inter/len(s1.union(s2))\n",
    "    elif mode==\"sim2paper\":\n",
    "        return path_inter/(2*len(s1)) + path_inter/(2*len(s2))\n",
    "    elif mode==\"sim3paper\":\n",
    "        return np.sqrt(path_inter**2 / (len(s1)*len(s2)))\n",
    "    elif mode==\"max_norm_sim\":\n",
    "        return path_inter/(max([len(s1), len(s2)]))\n",
    "    elif mode==\"min_norm_sim\":\n",
    "        return path_inter/(min([len(s1), len(s2)]))\n",
    "    else:\n",
    "        raise NotImplementedError(\"mode wrong, not implemented yet\")\n",
    "\n",
    "def k_shortest_paths(self, source, dest, k, overlap=0.5, mode=\"myset\"):\n",
    "    tic = time.time()\n",
    "    # initialize list of paths\n",
    "    sp_set = set(self.best_path)\n",
    "    best_paths = [self.best_path]\n",
    "    best_path_sets = [set(self.best_path)]\n",
    "    # get list of vertices = unique values in pos2node except -1\n",
    "    vertices = np.unique(self.pos2node)[1:]\n",
    "    v_dists = [self.dist_map_ab[v] + self.dist_map_ba[v] for v in vertices]\n",
    "    # sort paths\n",
    "    v_shortest = np.argsort(v_dists)\n",
    "    # iterate over vertices starting from shortest paths\n",
    "    # times_getpath = []\n",
    "    for j, v_ind in enumerate(v_shortest):\n",
    "        v = vertices[v_ind]\n",
    "        # TODO: for runtime scan only every xth one (anyways diverse)\n",
    "        if v not in sp_set:\n",
    "            # do not scan unreachable vertices\n",
    "            if int(self.pred_map_ab[v]\n",
    "                   ) == int(v) or int(self.pred_map_ba[v]) == int(v):\n",
    "                continue\n",
    "            # tic1 = time.time()\n",
    "            try:\n",
    "                path_ac = self.get_sp_from_preds(\n",
    "                    self.pred_map_ab, v, source\n",
    "                )\n",
    "                path_cb = self.get_sp_from_preds(self.pred_map_ba, v, dest)\n",
    "            except RuntimeWarning:\n",
    "                print(\"while loop not terminating\")\n",
    "                continue\n",
    "            # times_getpath.append(time.time() - tic1)\n",
    "            path_ac.reverse()\n",
    "            # concatenate - leave 1 away because otherwise twice\n",
    "            vertices_path = path_ac + path_cb[1:]\n",
    "            \n",
    "            # similar = similarity(vertices_path, best_paths, sp_set)\n",
    "            if mode!=\"myset\":\n",
    "                sofar = np.array([similarity(sp, set(vertices_path), mode) for sp in best_path_sets])\n",
    "                if np.all(sofar<overlap):\n",
    "                    best_paths.append(vertices_path)\n",
    "                    best_path_sets.append(set(vertices_path))\n",
    "            # mode myset --> my version: set of all paths together \n",
    "            else:\n",
    "                already = np.array([u in sp_set for u in vertices_path])\n",
    "                if np.sum(already) < len(already) * overlap:\n",
    "                    best_paths.append(vertices_path)\n",
    "                    sp_set.update(vertices_path)\n",
    "                # print(\"added path, already scanned\", j)\n",
    "        # stop if k paths are sampled\n",
    "        if len(best_paths) >= k:\n",
    "            break\n",
    "\n",
    "    self.time_logs[\"ksp\"] = round(time.time() - tic, 3)\n",
    "    return [self.transform_path(p) for p in best_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_shortest_path_tree(source_v, target_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksp = k_shortest_paths(graph, source_v, target_v, 10, overlap=0.5, mode=\"sim3paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([k[2] for k in ksp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all similarity metrics with threshold 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"myset\", \"IoU\",\"sim2paper\", \"sim3paper\", \"max_norm_sim\", \"min_norm_sim\"]:\n",
    "    ksp = k_shortest_paths(graph, source_v, target_v, 10, overlap=0.5, mode=mode)\n",
    "    print(\"---------------   MODE: \", mode, \"-------------------\")\n",
    "    plot_k_sp(ksp, graph.instance, out_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all similarity metrics with threshold 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mode in [\"myset\", \"IoU\",\"sim2paper\", \"sim3paper\", \"max_norm_sim\", \"min_norm_sim\"]:\n",
    "    ksp = k_shortest_paths(graph, source_v, target_v, 10, overlap=0.3, mode=mode)\n",
    "    print(\"---------------   MODE: \", mode, \"-------------------\")\n",
    "    plot_k_sp(ksp, graph.instance, out_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "* very similar results in general\n",
    "* IoU requires lower threshold than others (leading to less diverse paths)\n",
    "* min and max norm do differ slightly in this example\n",
    "* my set does only differ from others in this example by exchanging two paths (-->80% paths are same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import math\n",
    "\n",
    "\n",
    "N = 10\n",
    "space = np.zeros((N, N), dtype=np.int8)\n",
    "space[3:7, 3:7].fill(1)\n",
    "print(space)\n",
    "print(np.sum(space))\n",
    "\n",
    "space_coo = scipy.sparse.coo_matrix(space)\n",
    "Coords = np.array(space_coo.nonzero()) - 3\n",
    "\n",
    "theta = 30 * 3.1416 / 180\n",
    "\n",
    "R = np.array([[math.cos(theta), math.sin(theta)], [-math.sin(theta), math.cos(theta)]])\n",
    "space2_coords = R.dot(Coords)\n",
    "space2_coords = np.round(space2_coords)\n",
    "space2_coords += 3\n",
    "space2_sparse = scipy.sparse.coo_matrix(([1] * space2_coords.shape[1], (space2_coords[0], space2_coords[1])), shape=(N, N))\n",
    "space2 = space2_sparse.todense()\n",
    "print(space2)\n",
    "print(np.sum(space2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
