{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook contains:\n",
    "\n",
    "* Compare bellman ford to BF moore\n",
    "* attempts for distance cost from the actual path\n",
    "* Add emergency points only where necessary (dilation)\n",
    "* Agglomerative clustering\n",
    "* Build graph from clustered pos2node\n",
    "* Watershed transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import rasterio\n",
    "import networkx as nx\n",
    "import time\n",
    "from graph_tool.all import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_planner.utils.utils import get_half_donut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../data/instance_belgium/tif_layers/Buildingftp.tif\"\n",
    "with rasterio.open(path, 'r') as ds:\n",
    "    arr = ds.read()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOPATH = \"../data/data_dump_5.dat\"\n",
    "with open(IOPATH, \"rb\") as infile:\n",
    "    (instance, instance_corr, start_inds, dest_inds) = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(instance.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emerg = np.zeros((20,20))\n",
    "max_dist = 1.5\n",
    "for row in np.arange(1,20,max_dist).astype(int):\n",
    "    emerg[row, np.arange(1,20,max_dist).astype(int)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slow version dist arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inds = np.asarray([163, 24])\n",
    "dest_inds = np.asarray([94, 240])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_arr = np.zeros(instance.shape)\n",
    "norm = np.linalg.norm(dest_inds-start_inds)\n",
    "for i in range(len(dist_arr)):\n",
    "    for j in range(len(dist_arr[0])):\n",
    "        p3 = np.asarray([i,j])\n",
    "        dist_arr[i,j] = np.abs(np.cross(dest_inds-start_inds, start_inds-p3)) / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dist_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fast version for dist arr, but rotation padding problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = np.linalg.norm(dest_inds-start_inds)\n",
    "size = int(size)\n",
    "arr = np.zeros((size,size))\n",
    "arr[:,:size//2] = np.stack([np.arange(size//2) for _ in range(size)])\n",
    "arr[:,size//2:] = np.vstack([np.arange(size//2,0,-1).tolist() for _ in range(size)])\n",
    "ang = 90 + 90 * angle([0,1], dest_inds-start_inds)/np.pi\n",
    "w,h = instance.shape\n",
    "dist_arr = np.pad(rotate(arr, ang), ((0, w-size), (0,h-size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## construct constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_inds = [163, 24]\n",
    "dest_inds = [94, 240]\n",
    "start_dest_inds = np.array([start_inds, dest_inds])\n",
    "inter_line = start_dest_inds[0]-start_dest_inds[1]\n",
    "longer = np.argmin(np.abs(inter_line))\n",
    "\n",
    "padding = [0,0]\n",
    "percent_padding = 0.25\n",
    "padding[longer] = abs(int(percent_padding*inter_line[longer]))\n",
    "\n",
    "start_x, start_y = np.min(start_dest_inds, axis=0) - np.asarray(padding)\n",
    "end_x, end_y = np.max(start_dest_inds, axis=0) + np.asarray(padding)\n",
    "\n",
    "corr = np.zeros(instance.shape)\n",
    "corr[start_x:end_x, start_y:end_y] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_cons = corr * instance_corr # hard_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dist = 10\n",
    "w_inds = np.arange(start_x, end_x, max_dist).astype(int)\n",
    "h_inds = np.arange(start_y, end_y, max_dist).astype(int)\n",
    "\n",
    "# max_cost = np.max(costs)\n",
    "for row in w_inds:\n",
    "    hard_cons[row, h_inds] = 1\n",
    "    # costs[row, h_inds] = max_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x, start_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(hard_cons) # possible)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test algorithms on instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INP = \"../../outputs/path_70055.weighted.edgelist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_edgelist(\n",
    "                INP,\n",
    "                nodetype=float,\n",
    "                create_using=nx.DiGraph,\n",
    "                data=(('weight', float), )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.linalg.norm([-173, 539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 4*vec/15\n",
    "cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SP():\n",
    "    def __init__(self, graph):\n",
    "        self.graph = graph\n",
    "\n",
    "    def bellman_ford_my(self, source, target, cutoff):\n",
    "        \"\"\"\n",
    "        Actual BF algorithm, not SPFA\n",
    "        \"\"\"\n",
    "        pred = {}\n",
    "        dist = {source: 0}\n",
    "\n",
    "        inf = float('inf')\n",
    "\n",
    "        for i in range(int(cutoff)):\n",
    "            print(i)\n",
    "            for (u, v, w_dict) in self.graph.edges(data=True):\n",
    "                w = w_dict[\"weight\"]\n",
    "                if dist.get(u, inf) + w < dist.get(v, inf):\n",
    "                    dist[v] = dist[u] + w\n",
    "                    pred[v] = u\n",
    "        path = [target]\n",
    "        curr = target\n",
    "        while curr != source:\n",
    "            curr = pred[curr]\n",
    "            path.append(curr)\n",
    "        path.append(source)\n",
    "        return list(reversed(path))\n",
    "\n",
    "    def bellman_ford_nx(\n",
    "        self,\n",
    "        source,\n",
    "        target,\n",
    "        pred=None,\n",
    "        paths=None,\n",
    "        dist=None,\n",
    "    ):\n",
    "        \"\"\"Relaxation loop for Bellman–Ford algorithm.\n",
    "        This is an implementation of the SPFA variant.\n",
    "        See https://en.wikipedia.org/wiki/Shortest_Path_Faster_Algorithm\n",
    "        Parameters\n",
    "        SEE https://github.com/networkx/networkx/blob/02a1721276b3a84d3be8558e4\n",
    "        79a9cb6b0715488/networkx/algorithms/shortest_paths/weighted.py#L1203\n",
    "        \"\"\"\n",
    "        G = self.graph\n",
    "        weight = lambda u, v, data: data.get(\"weight\", 1)\n",
    " \n",
    "        # vertices_path = self.bellman_ford(source, target, cutoff)\n",
    "        paths = {source: [source]}\n",
    "        # dist, it_list = self._bellman_ford(self.graph, [source], weight, cutoff, paths = paths, target=target)\n",
    "        # vertices_path = paths[target]\n",
    "        source = [source]\n",
    "        \n",
    "        for s in source:\n",
    "            if s not in G:\n",
    "                raise nx.NodeNotFound(f\"Source {s} not in G\")\n",
    "\n",
    "        if pred is None:\n",
    "            pred = {v: [] for v in source}\n",
    "\n",
    "        if dist is None:\n",
    "            dist = {v: 0 for v in source}\n",
    "\n",
    "        G_succ = G.succ if G.is_directed() else G.adj\n",
    "        print(\"directed?\", G.is_directed())\n",
    "        inf = float('inf')\n",
    "        n = len(G)\n",
    "\n",
    "        # count = {}\n",
    "        q = deque(source)\n",
    "        in_q = set(source)\n",
    "        iteration = 0\n",
    "        it_list = []\n",
    "        while q:\n",
    "            u = q.popleft()\n",
    "            in_q.remove(u)\n",
    "            \n",
    "            it_ind = 0\n",
    "            \n",
    "            # print(\"new vertex\", u)\n",
    "\n",
    "            # Skip relaxations if any of the predecessors of u is in the queue.\n",
    "            if all(pred_u not in in_q for pred_u in pred[u]):\n",
    "                dist_u = dist[u]\n",
    "                # print(dist_u)\n",
    "                for v, e in G_succ[u].items():\n",
    "                    dist_v = dist_u + weight(u, v, e)  # TODO:replace function\n",
    "                    # print(v)\n",
    "                    if dist_v < dist.get(v, inf):\n",
    "                        \n",
    "                        if v not in in_q:\n",
    "                            # print(\"update\")\n",
    "                            q.append(v)\n",
    "                            in_q.add(v)\n",
    "                            # count_v = count.get(v, 0) + 1\n",
    "                            # if count_v == n:\n",
    "                            #     raise nx.NetworkXUnbounded(\n",
    "                            #         \"Negative cost cycle detected.\"\n",
    "                            #     )\n",
    "                            # count[v] = count_v\n",
    "                        dist[v] = dist_v\n",
    "                        pred[v] = [u]\n",
    "\n",
    "                    elif dist.get(v) is not None and dist_v == dist.get(v):\n",
    "                        pred[v].append(u)\n",
    "                    \n",
    "                    it_ind += 1\n",
    "            else:\n",
    "                # pass\n",
    "                print(\"err\")\n",
    "            \n",
    "            it_list.append(it_ind)\n",
    "            iteration += 1\n",
    "\n",
    "            # TODO\n",
    "            # if u == target:\n",
    "            #     print(\"early stopping\")\n",
    "            #     break\n",
    "            # if iteration > cutoff and dist.get(target, inf) < inf:\n",
    "                # print(\"iteration more than cutoff\")\n",
    "                # if dist.get(target, inf) < inf:\n",
    "                # print(\"early stopping\")\n",
    "                # break\n",
    "        print(\"number of iterations:\", iteration)\n",
    "        if paths is not None:\n",
    "            dsts = [target] if target is not None else pred\n",
    "            for dst in dsts:\n",
    "\n",
    "                path = [dst]\n",
    "                cur = dst\n",
    "\n",
    "                while pred[cur]:\n",
    "                    cur = pred[cur][0]\n",
    "                    path.append(cur)\n",
    "\n",
    "                path.reverse()\n",
    "                paths[dst] = path\n",
    "\n",
    "        return paths[target], it_list # dist, it_list\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_undir = g.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Start shortest path\")\n",
    "sp = SP(g)\n",
    "out = sp.bellman_ford_nx(43104, 8447) # , it_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results:\n",
    "\n",
    "* graph with 5.6 mio edges, 50 000 nodes\n",
    "* directed case: 50757 terations, so slightly more than the nodes --> doch cycles --> maybe because of 180 degrees thing? (15 sec)\n",
    "* undirected case: 59042 iterations why not more? though more iterations in each iteration probably, takes longer (26 sec)\n",
    "\n",
    "\n",
    "* my algorithm: 10 minutes! (for 150 iterations) --> basically 150 times the runtime of the other ones\n",
    "* with directed edges, other algorithm: 5 837 055, but just twice the time 12 753 072 if undirected\n",
    "\n",
    "\n",
    "* because of Skip relaxations if any of the predecessors of u is in the queue?\n",
    "\n",
    "sanity check was correct: inner iteration (over neighbors) is twice as much for undirected as for directed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(g.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total iterations for directed / undirected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(50757*115, 59042*216)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(it_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(it_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(it_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(it_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random graph tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = nx.complete_graph(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SP(comp)\n",
    "out, its = sp.bellman_ford_nx(0, 5) # , it_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(its)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new = nx.DiGraph()\n",
    "e = []\n",
    "# for i in range(20):\n",
    "#    e.append([i,i+1,{\"weight\":i}])\n",
    "for i in range(1,4):\n",
    "    e.append([0,i,{\"weight\":i}])\n",
    "for i in range(1,4):\n",
    "    e.append([i,5, {\"weight\":i}])\n",
    "g_new.add_edges_from(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SP(g_new)\n",
    "out = sp.get_shortest_path_nx(0, 5) # , it_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_weighted_edgelist(\n",
    "                g_new,'test.weighted.edgelist'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_g_new = nx.read_edgelist(\n",
    "                'test.weighted.edgelist',\n",
    "                nodetype=int,\n",
    "    create_using=nx.DiGraph,\n",
    "                data=(('weight', float), )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_g_new_dir = nx.DiGraph(in_g_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_g_new.out_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_new.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comp.nodes()), len(comp.edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = Graph()\n",
    "g1.add_vertex(20)\n",
    "g1.add_edge_list([[i,i+1] for i in range(19)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(g1.vertices())), len(list(g1.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(comp.nodes())), len(list(comp.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subplots for paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((500,1000))\n",
    "a[200:260, 100:500] = np.arange(24000).reshape(60,400)\n",
    "a = np.swapaxes(a,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[:, np.any(a>0, axis=0)]\n",
    "b = b[np.any(b>0, axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(20).reshape(2,10)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,15))\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    plt.imshow(b)\n",
    "    plt.axis('scaled')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = instance_corr * instance\n",
    "test = inst[:, np.any(inst > 0, axis=0)]\n",
    "test = test[np.any(test > 0, axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can put same node in several places of pos2node\n",
    "# summarize pixels: with similar value?\n",
    "# from image compression\n",
    "# put gaussian smoothing on top? --> then take similar ones together?\n",
    "# problem clustering: what about different resistance classes\n",
    "# distance constraint? take highest or lowest distance to cluster\n",
    "# different resistance classes = colour channels\n",
    "# graph representation: make edges strong between similar values, then find cuts\n",
    "# take position and costs together and do kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(img, scale_factor):\n",
    "    x_len_new = img.shape[0] // scale_factor\n",
    "    y_len_new = img.shape[1] // scale_factor\n",
    "    new_img = np.zeros((x_len_new, y_len_new))\n",
    "    for i in range(x_len_new):\n",
    "        for j in range(y_len_new):\n",
    "            patch = img[i * scale_factor:(i + 1) *\n",
    "                        scale_factor, j *\n",
    "                        scale_factor:(j + 1) * scale_factor]\n",
    "            new_img[i, j] = np.mean(patch)\n",
    "    return np.swapaxes(new_img, 1, 0)\n",
    "\n",
    "inst = reduce(test, 20)\n",
    "# inst = (inst>0.4).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.image import grid_to_graph\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothened_coins = inst # gaussian_filter(inst, sigma=1)\n",
    "\n",
    "X = np.reshape(smoothened_coins, (-1, 1))\n",
    "\n",
    "# #############################################################################\n",
    "# Define the structure A of the data. Pixels connected to their neighbors.\n",
    "connectivity = grid_to_graph(*smoothened_coins.shape)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute clustering\n",
    "print(\"Compute structured hierarchical clustering...\")\n",
    "st = time.time()\n",
    "n_clusters = 20  # number of regions\n",
    "ward = AgglomerativeClustering(n_clusters=None, linkage='complete', affinity =\"l1\", distance_threshold=0.2,\n",
    "                               connectivity=connectivity) # n_clusters=n_clusters,\n",
    "ward.fit(X)\n",
    "label = np.reshape(ward.labels_, smoothened_coins.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(inst)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(label, interpolation='nearest', cmap=plt.cm.nipy_spectral) # , cmap=plt.cm.gray)\n",
    "# for l in range(n_clusters):\n",
    "#     plt.contour(label == l,\n",
    "#                 colors=[plt.cm.nipy_spectral(l / float(n_clusters)), ])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(inst, n_clusters, plot=True):\n",
    "\n",
    "    X = np.reshape(inst, (-1, 1))\n",
    "\n",
    "    # Define the structure A of the data. Pixels connected to their neighbors.\n",
    "    connectivity = grid_to_graph(*inst.shape)\n",
    "\n",
    "    # print(\"Compute structured hierarchical clustering...\")\n",
    "    st = time.time()\n",
    "    ward = AgglomerativeClustering(n_clusters=n_clusters, linkage='complete', affinity =\"euclidean\", # l1\",\n",
    "                                   connectivity=connectivity)\n",
    "    ward.fit(X)\n",
    "    label = np.reshape(ward.labels_, inst.shape)\n",
    "    # print(\"time passed\", time.time()-st)\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(inst)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(label, interpolation='nearest', cmap=plt.cm.nipy_spectral)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = instance_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unterteile bild in several parts, compute clusters for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2node  = label # just saying for each one where we have the corresponding\n",
    "new cost instance = # mean of patch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x):\n",
    "    for j in range(y):\n",
    "        if instance_corr[i,j] == instance_corr[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "\n",
    "def split_to_shape(a, chunk_shape, start_axis=0):\n",
    "    if len(chunk_shape) != len(a.shape):\n",
    "        raise ValueError('chunk length does not match array number of axes')\n",
    "\n",
    "    if start_axis == len(a.shape):\n",
    "        return a\n",
    "\n",
    "    num_sections = math.ceil(a.shape[start_axis] / chunk_shape[start_axis])\n",
    "    # print(num_sections)\n",
    "    split = numpy.array_split(a, num_sections, axis=start_axis)\n",
    "    return [split_to_shape(split_a, chunk_shape, start_axis + 1) for split_a in split]\n",
    "\n",
    "full_split = split_to_shape(test, (3,3))\n",
    "print({i2.shape for i in full_split for i2 in i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_vals = len(np.unique(test))\n",
    "compress = 2\n",
    "\n",
    "out = np.zeros(test.shape)\n",
    "labs_start = 0\n",
    "x_done = 0\n",
    "for row_splits in full_split:\n",
    "    y_done = 0\n",
    "    for col_splits in row_splits:\n",
    "        if np.any(col_splits):\n",
    "            x,y = col_splits.shape\n",
    "            # distinct_vals = len(np.unique(col_splits))\n",
    "            # out_labs = int(np.ceil(distinct_vals/compress))\n",
    "            # print(\"out labs\", out_labs)\n",
    "            out_labs = 2\n",
    "            labs = clustering(col_splits, out_labs, plot=False)\n",
    "            out[x_done:x_done+x, y_done:y_done+y] = labs + labs_start\n",
    "            y_done += y\n",
    "            labs_start += np.max(labs)+1\n",
    "    x_done+= x\n",
    "    # labs = clustering(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(test)\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out[99:102, 63:66])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_planner.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = get_half_donut(3,5,[1,1])\n",
    "# pos2node = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_constraints = np.zeros(out.shape)\n",
    "hard_constraints[5:-5, 5:-5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_vertices = out[hard_constraints>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_vertices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs_shifted = shift_surface(test, shifts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (costs_shifted + test)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_shifted = shift_surface(pos2node.copy(), shifts[0])[hard_constraints>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(prev_vertices), len(nodes_shifted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = weights[hard_constraints>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds_arr = np.asarray([prev_vertices, nodes_shifted, weights_list])\n",
    "inds_arr.shape\n",
    "# inds_weights = np.concatenate((inds_arr, weights_arr), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df = pd.DataFrame(np.swapaxes(inds_arr, 1, 0), columns=[\"1\", \"2\", \"3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop_duplicates()\n",
    "df = df[df[\"3\"]>0] # cost greater zero --> cannot be both forbidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"1\"]!=df[\"2\"]] # not from one node to itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.where(out==195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.groupby([\"1\",\"2\"], as_index=False).agg({\"3\":\"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "125*243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_planner.utils.utils import get_half_donut, shift_surface\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2node = out.copy()\n",
    "# instance = test.copy()\n",
    "hard_constraints = (test>0).astype(int)\n",
    "# instance[instance==0] = 1\n",
    "shifts = get_half_donut(3,5,[1,1])\n",
    "max_cost = np.max(instance)\n",
    "print(max_cost)\n",
    "d = 2\n",
    "tic = time.time()\n",
    "emerg_counter = np.max(pos2node)\n",
    "print(emerg_counter)\n",
    "for i in range(d, len(hard_constraints)-d):\n",
    "    for j in range(d, len(hard_constraints[0])-d):\n",
    "        if not np.any(hard_constraints[i - d:i + d, j - d:j + d]):\n",
    "            hard_constraints[i, j] = 1\n",
    "            test[i, j] = max_cost\n",
    "            pos2node[i,j] = emerg_counter\n",
    "            emerg_counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pos2node)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_vertex_ind = pos2node[100,25]\n",
    "dest_vertex_ind = pos2node[20,240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = len(np.unique(pos2node))\n",
    "_ = g.add_vertex(n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = g.new_edge_property(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inds = pos2node>0\n",
    "x_inds, y_inds = np.where(inds)\n",
    "print(np.mean(x_inds), np.mean(y_inds))\n",
    "\n",
    "np.mean(np.vstack([x_inds, y_inds]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_vertices = pos2node[hard_constraints>0]\n",
    "for shift in shifts[:1]:\n",
    "    costs_shifted = shift_surface(test, shift)\n",
    "    weights = (costs_shifted + test)/2\n",
    "    nodes_shifted = shift_surface(pos2node.copy(), shift)[hard_constraints>0]\n",
    "    weights_list = weights[hard_constraints>0]\n",
    "    inds_arr = np.asarray([prev_vertices, nodes_shifted, weights_list])\n",
    "    # print(inds_arr.shape)\n",
    "    df = pd.DataFrame(np.swapaxes(inds_arr, 1, 0), columns=[\"1\", \"2\", \"3\"])\n",
    "    print(shift)\n",
    "    print(df.head(20))\n",
    "    df = df[df[\"3\"]>0]\n",
    "    df = df[df[\"2\"]>0]\n",
    "    df = df[df[\"1\"]>0]\n",
    "    df = df[df[\"1\"]!=df[\"2\"]]\n",
    "    df = df.groupby([\"1\",\"2\"], as_index=False).agg({\"3\":\"sum\"})\n",
    "    # problem: only summing up the ones going to the same pair --> 163 to 195 can have low weights, but there can be 1000 points in cluster 163\n",
    "    edges = np.array(df)\n",
    "    g.add_edge_list(edges, eprops=[weight])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_path, _ = shortest_path(\n",
    "                g,\n",
    "                g.vertex(start_vertex_ind),\n",
    "                g.vertex(dest_vertex_ind),\n",
    "                weights=weight,\n",
    "                negative_weights=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_map = np.zeros(out.shape)\n",
    "col = 1\n",
    "for v in vertices_path:\n",
    "    v_ind = g.vertex_index[v]\n",
    "    inds = np.where(pos2node==v_ind)\n",
    "    min_val = 1\n",
    "    for (i,j) in zip(inds[0], inds[1]):\n",
    "        # print(test[i,j])\n",
    "        if test[i,j] < min_val:\n",
    "            min_val = test[i,j]\n",
    "            min_ind_x = i\n",
    "            min_ind_y = j\n",
    "    # print(v_ind)\n",
    "    path_map[min_ind_x, min_ind_y]= col\n",
    "    col += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(path_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"1\"]==143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.where(out==143)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inds = (t[0]-3, t[1]+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i,j) in zip(new_inds[0], new_inds[1]):\n",
    "    o = out[i,j]\n",
    "    if o==195:\n",
    "        print(weights[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill random points for empty areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = (test<1).astype(int).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix: only better emergency points for parcels:\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(example)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im verhältnis zu number of pixels in this part\n",
    "dist = 3\n",
    "x,y = example.shape\n",
    "for i in range(dist,x-dist):\n",
    "    for j in range(dist,y-dist):\n",
    "        if ex2[i,j]:\n",
    "            if not np.any(example[i-dist:i+dist, j-dist:j+dist]):\n",
    "                example[i,j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.morphology import binary_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2 = binary_dilation(example, iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## watershed segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage as ndi\n",
    "\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage import data, util, filters, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def watershed_transform(img, n_clusters, compact=0.01):\n",
    "    greater_zero = (img>0).astype(int)\n",
    "    edges = filters.sobel(img)\n",
    "    \n",
    "    ratio_zero = np.sum(greater_zero)/len(greater_zero.flatten())\n",
    "    n_seeds = n_clusters/ratio_zero\n",
    "    \n",
    "    grid = util.regular_grid(img.shape, n_points=n_seeds)\n",
    "    seeds = np.zeros(img.shape, dtype=int)\n",
    "    seeds[grid] = np.arange(seeds[grid].size).reshape(seeds[grid].shape) + 1\n",
    "    seeds = seeds*greater_zero\n",
    "    print(\"number seeds then\", np.sum(seeds>0))\n",
    "    \n",
    "    w1 = watershed(edges, seeds, compactness=compact)\n",
    "    \n",
    "    w1_g_zero = (w1+1)*greater_zero\n",
    "    labels = np.unique(w1_g_zero)\n",
    "    transformed = np.zeros(w1.shape).astype(int)\n",
    "    nr_members = np.zeros(w1.shape).astype(int)\n",
    "    for i,lab in enumerate(labels):\n",
    "        inds = w1_g_zero==lab\n",
    "        transformed[inds] = i\n",
    "        nr_members[inds] = np.sum(inds)\n",
    "    \n",
    "    return transformed, nr_members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_cols(arr):\n",
    "    uni = np.unique(arr)\n",
    "    transformed = np.zeros(int(np.max(uni)+1))\n",
    "    for i, u in enumerate(uni):\n",
    "        transformed[int(u)] = i\n",
    "    cols = np.random.rand(len(uni),3)\n",
    "    x,y = arr.shape\n",
    "    new = np.zeros((x,y,3))\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            new[i,j] = cols[int(transformed[int(arr[i,j])])]\n",
    "    return new   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = instance_corr * instance\n",
    "test = inst[:, np.any(inst > 0, axis=0)]\n",
    "test = test[np.any(test > 0, axis=1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take weighted sum of different cost surfaces --> this is what needs to be similar in the end?\n",
    "out, members = watershed_transform(test,5000)\n",
    "cols = transform_cols(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(cols[90:100, 60:70])\n",
    "plt.show()\n",
    "# print(w0[90:100, 60:70])\n",
    "plt.imshow(test[90:100, 60:70])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(out), len(np.unique(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni, cou = np.unique(members, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cou[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"no nodes now\", len(np.unique(out)), \"previously <0\", np.sum(test>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(img, scale_factor):\n",
    "    x_len_new = img.shape[0] // scale_factor\n",
    "    y_len_new = img.shape[1] // scale_factor\n",
    "    new_img = np.zeros((x_len_new, y_len_new))\n",
    "    std_img = np.zeros((x_len_new, y_len_new))\n",
    "    for i in range(x_len_new):\n",
    "        for j in range(y_len_new):\n",
    "            patch = img[i * scale_factor:(i + 1) *\n",
    "                        scale_factor, j *\n",
    "                        scale_factor:(j + 1) * scale_factor]\n",
    "            new_img[i, j] = np.mean(patch)\n",
    "            std_img[i,j] = np.std(patch)\n",
    "    return np.swapaxes(new_img, 1, 0), np.swapaxes(std_img, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red, stds = reduce(test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"actual reduce function number >0:\", np.sum(red>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members[members>1000] = 0\n",
    "print(np.mean(members[members>0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(members)/5.5685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds_out = []\n",
    "for i, val in enumerate(np.unique(out)):\n",
    "    if val==0:\n",
    "        continue\n",
    "    vals = test[out==val]\n",
    "    stds_out.append(np.std(vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stds_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_stds = stds[red>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(actual_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(stds_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(actual_stds, bins = np.arange(0,0.25,0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stds_out, bins = np.arange(0,0.25,0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time of watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_planner.utils.utils import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_layers = \"tif_layers\"\n",
    "cost_arr = []\n",
    "for f in os.listdir(path_layers):\n",
    "    if f[-3:]==\"tif\":\n",
    "        with rasterio.open(path_layers+\"/\"+f, 'r') as ds:\n",
    "            arr = ds.read()\n",
    "        if arr.shape==(1, 1313, 1511):\n",
    "            cost_arr.append(normalize(arr[0]))\n",
    "cost_arr = np.array(cost_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface = normalize(np.sum(cost_arr, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red5, _ = reduce(surface, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(red5)) # much more because taken mean when reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(surface)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "out, members = watershed_transform(surface, 250000, compact=0.01)\n",
    "print(\"Time for scale\", scale,\":\", round(time.time() - tic,3), \"(shape:\", red5.shape, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "Seeds: 5000\n",
    "\n",
    "* Time for scale 5 : 1.185 (shape: (302, 262) )\n",
    "* Time for scale 4 : 1.165  )\n",
    "* Time for scale 2 : 6.658 (shape: (755, 656) )\n",
    "* Time for full : 30.538 (shape: (1313, 1511) )\n",
    "\n",
    "Seeds: 10000\n",
    "\n",
    "Time for scale 2 : 14.146 (shape: (755, 656) )\n",
    "\n",
    "Seeds: 20000\n",
    "\n",
    "Time for scale 2 : 25.454 (shape: (755, 656) )  (ACTUALLY: would be 500 000 vertices)\n",
    "\n",
    "Seeds: 31000\n",
    "\n",
    "Time for scale 2 : 41.408 (shape: (755, 656) )\n",
    "\n",
    "Seeds: 40607\n",
    "\n",
    "Time for scale 1 : 207.016 (shape: (755, 656) ) --> already 3 min (ACTUALLY 1.9 vertices) \n",
    "\n",
    "Seeds 220 000 --> 1/8 th of all vertices are kept:\n",
    "\n",
    "Time for scale 1: 1138 sec = 19 min\n",
    "\n",
    "--> Seems to scale linear with number of seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1313 * 1511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(test>0)/len(test.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "302*262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_seeds = 3000\n",
    "grid = util.regular_grid(test.shape, n_points=n_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seeds = np.zeros(test.shape, dtype=int)\n",
    "seeds[grid] = np.arange(seeds[grid].size\n",
    "                                ).reshape(seeds[grid].shape) + 1\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(new)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_scale = 5\n",
    "lab = 0\n",
    "x_len, y_len = test.shape\n",
    "new = np.zeros(test.shape)\n",
    "for i in np.arange(0,x_len, cluster_scale):\n",
    "    for j in np.arange(0,y_len, cluster_scale):\n",
    "        if test[i,j]:\n",
    "            new[i,j] = lab\n",
    "            lab += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
