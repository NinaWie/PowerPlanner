{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook contains:\n",
    "\n",
    "* Timing tests notes\n",
    "* Plotting tool to compare different paths, and plot time logs\n",
    "* Fitting piecewise linear function\n",
    "* reading in the tif with weights\n",
    "* Faster edge convolutions\n",
    "* read start and dest from tifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import networkx as nx\n",
    "import os\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import time\n",
    "import json\n",
    "from graph_tool.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_planner.utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next meeting:\n",
    "\n",
    "* path straightening in constraints notebook with algorithm is not an actual straightening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain pipeline:\n",
    "\n",
    "* Get hard constraints --> binary corridor, + water etc\n",
    "* Add one node to the graph for each raster cell in the corridor\n",
    "* Define donut around each cell\n",
    "* Add edges between each point and the point in the donut around it, with weights:\n",
    "    * First option: sum of cost of both cells of these two nodes\n",
    "    * Second option: sum of cost of all cells inbetween --> Bresenham line (other notebook)\n",
    "* Add start and end node, and connect to the top x and bottom x points of the corridor with weight 0\n",
    "* Run shortest path algorithm (bellman ford)\n",
    "\n",
    "So far: downscaling with different factors\n",
    "\n",
    "### Runtimes:\n",
    "\n",
    "**Scale by 5 --> 20 poins in donut**\n",
    "* edge convolutions: {'init_graph': 0.03, 'add_nodes': 1.676, **'edge_list (per convolution!)': 0.9, 'add_edges (per shift)': 0.0045**, 'start_end_vertex': 0.004, 'shortest_path': 0.363}\n",
    "* node adding: {'init_graph': 0.03, 'add_nodes': 1.376, **'edge_list': 0.00245, 'add_edges': 0.00665,** 'start_end_vertex': 0.005, 'shortest_path': 0.236}\n",
    "* old edge version {'init_graph': 0.031, 'add_nodes': 1.481, **'edge_list': 0.198, 'add_edges': 0.045**, 'start_end_vertex': 0.003, 'shortest_path': 0.216}\n",
    "\n",
    "--> add adges even longer if list not split?!, improved node constraints by factor of 80\n",
    "\n",
    "**networkx scale 5:**\n",
    "* bellman ford: {'init_graph': 0.037, 'add_nodes': 0.03, 'edge_list': 0.002, **'add_edges': 0.2996**, 'start_end_vertex': 0.007, **'shortest_path': 2.961**}\n",
    "* dijkstra: {'init_graph': 0.036, 'add_nodes': 0.03, 'edge_list': 0.0020000000000000005, 'add_edges': 0.2926000000000001, 'start_end_vertex': 0.004, 'shortest_path': 0.633}\n",
    "\n",
    "--> graph-tool speed up: shortest path 10 times (bellman ford), 2 times (dijkstra), adding edges 60 times\n",
    "\n",
    "**Scale by 2 --> 156 point in donut**\n",
    "* edge convolutions with **faster convolutions**: {'init_graph': 0.233, 'add_nodes': 0.005, 'concatenate': 0.328, 'add_edges': 0.025, **'edge_list': 0.15**, 'start_end_vertex': 0.015, 'shortest_path': 27.296}\n",
    "* edge convolutions: {'init_graph': 0.198, 'add_nodes': 9.569, **'edge_list': 6.908, 'add_edges': 0.0273**, 'start_end_vertex': 0.034, 'shortest_path': 32.844}\n",
    "* node adding: {'init_graph': 0.202, 'add_nodes': 9.458, **'edge_list': 0.0092, 'add_edges': 0.024,** 'start_end_vertex': 0.017, 'shortest_path': 7.76}\n",
    "* node adding graph tool dijkstra: {'init_graph': 0.274, 'add_nodes': 0.006, 'edge_list': 0.010, 'add_edges': 0.029, 'start_end_vertex': 0.018, **'shortest_path': 0.822**}\n",
    "* old edge version: {'init_graph': 0.206, 'add_nodes': 10.979, **'edge_list': 1.154, 'add_edges': 0.314,** 'start_end_vertex': 0.032, 'shortest_path': 7.492}\n",
    "* networkx dijkstra: {'init_graph': 0.212, 'add_nodes': 0.156, 'edge_list': 0.01, **'add_edges': 3.08,** 'start_end_vertex': 0.065, **'shortest_path': 41.075**}\n",
    "\n",
    "--> speed up of node constraint by factor of 125\n",
    "--> graph tool speedup: 6 times for shortest path, \n",
    "\n",
    "**Whole instance: 428643 nodes --> around 263 615 445 edges**\n",
    "* dijkstra does not work because out of memory, time is {'init_graph': 0.624, 'add_nodes': 0.021, 'edge_list': 0.065,**'add_edges': 1.179**, 'start_end_vertex': 0.61,**'shortest_path': 611.761**}\n",
    "* with adding all edges at ones (bellman ford graph-tool): crashed \n",
    "\n",
    "**Directed graph:**\n",
    "* speed up for shortest path by a lot! \n",
    "* scale 2 directed: 0.5 \n",
    "* {'init_graph': 0.218, 'add_nodes': 0.005, 'add_edges': 0.024, 'edge_list': 0.12, **'shortest_path': 0.518**}\n",
    "* scale 2 undirected: 8\n",
    "\n",
    "**Other observations:**\n",
    "* graph-tool becomes slower for adding edges, the more edges are in the graph already\n",
    "* graph-tool with adding all edges at once: {'init_graph': 0.211, 'add_nodes': 0.005, 'add_edges': 0.026, 'edge_list': 0.017, 'start_end_vertex': 0.007, 'shortest_path': 6.554}\n",
    "* **add_nodes is much faster when no generator is returned (and seems to work just as fine**\n",
    "\n",
    "\n",
    "### NEW TEST:\n",
    "* whole instance {'init_graph': 0.626, 'add_nodes': 0.003, 'add_edges': 0.008, 'edge_list': 0.025, 'shortest_path': 6.268}\n",
    "\n",
    "### Space efficiency:\n",
    "\n",
    "* with dijkstra my laptop crashed, with bellman ford it's okay (for full instance --> 249 000 000 edges)\n",
    "* scaled 2 still fine 15 912 513 edges\n",
    "\n",
    "### Discuss:\n",
    "\n",
    "* Angle constraints: \n",
    "    * Path straightening (other notebook) is actually sampling fewer points\n",
    "    * Line graph takes m*1/3 d\n",
    "\n",
    "### Questions:\n",
    "\n",
    "Pipeline so far:\n",
    "\n",
    "* Normalization: 0-1 normalization for cost surface?\n",
    "* Bresenham line makes sense?\n",
    "* Start and end node given? first row?\n",
    "* networkx version\n",
    "\n",
    "Constraints:\n",
    "\n",
    "* Hard constraints: Distance from towns etc? Water?\n",
    "* View on towers / cables constraint?\n",
    "* Height of towers\n",
    "* Monetary costs? --> node costs in addition, so it's not placing too many towers\n",
    "\n",
    "How to use other tifs?? - Excel sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meeting:\n",
    "\n",
    "* infinity costs on edges for NaNs\n",
    "* function to compute resistance: multiply raster tifs with weights\n",
    "* the more negative the value, the more suitable\n",
    "* completely forbidden: NaN or infinite\n",
    "* MCDA negative values - but zickszack\n",
    "\n",
    "Todo:\n",
    "* flip coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtimes Linegraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Line graph: {'init_graph': 0.0, 'add_nodes': 1.7634360790252686, 'add_edges': 0.023, 'edge_list': 0.079, 'shortest_path': 15.495} for 878 iterations --> add_edges ca. 90 seconds\n",
    "* normal graph: {'init_graph': 0.038, 'add_nodes': 0.001, 'add_edges': 0.003, 'edge_list': 0.002, 'shortest_path': 0.162}\n",
    "* line graph from file:{'init_graph': 9.354, 'add_nodes': 0.028, 'add_edges': 48.366, 'edge_list': 902.766, 'shortest_path': 4.559}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Ford:\n",
    "\n",
    "* Normal bellman ford: for all nodes for all edges --> in each step, check all edges\n",
    "* for normal BF: We know that after iteration N, if there is a path from source to target with N edges, then this is a shortest path with N edges\n",
    "* how does this help? Idea: Approximate max number of pylons X. Then after X iterations, if there is a path of length X, it is the shortest path --> we could check all paths of length <= X, and take the shortest one of those.\n",
    "\n",
    "Problem: ITERATING THROUGH ALL EDGES is very expensive\n",
    "\n",
    "* very slow already for 450 000\n",
    "* other implementations just work with checking neighbors of current vertex --> SPFA implemented in networkx and probably also graph-tool\n",
    "* https://pdfs.semanticscholar.org/d75a/87a3232bef187e710ef59e202816187c8669.pdf try this: speed up of SPFA\n",
    "\n",
    "Idea: do SPFA, stop if target was considered --> gives same result as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_path = \"../outputs/compare\" # path_whole_instance_infos.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "files = []\n",
    "for f in os.listdir(compare_path):\n",
    "    if f[-4:]==\"json\":\n",
    "        print(f)\n",
    "        with open(os.path.join(compare_path,f),\"r\") as infile:\n",
    "            infos = json.load(infile)\n",
    "        paths.append(infos[\"path_cells\"])\n",
    "        files.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for p, f in zip(paths, files):\n",
    "    p_arr = np.array(p)\n",
    "    plt.scatter(p_arr[:,1], p_arr[:,0], label = f)\n",
    "    print(f, \"path length:\", len(p))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "* Way more nodes in edge cost path\n",
    "* Combining both --> edge cost seem to matter more (still 61 vs 52 nodes) --> would need to weight them somehow\n",
    "* idea: adapt donut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check different edge costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_path = \"../outputs/path_15860_infos.json\"\n",
    "instance_path = \"tifs_new/data_dump_5.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(info_path,\"r\") as infile:\n",
    "    infos = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgecosts = infos[\"edgecosts\"]\n",
    "edgecosts.insert(0, [0,0])\n",
    "edgecosts.append([0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(instance_path, \"rb\") as infile:\n",
    "    (instance, instance_corr, start_inds, dest_inds) = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_path = np.asarray(infos[\"path_cells\"]) // 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(scaled_path)-1):\n",
    "    (h,j) = tuple(scaled_path[i])\n",
    "    (k,l) = tuple(scaled_path[i+1])\n",
    "    print(scaled_path[i], scaled_path[i+1], edgecosts[i])\n",
    "    print(instance[h,j], instance[k,l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path_costs(instance, path, edgecosts, out_path = None, buffer=1):\n",
    "    expanded = np.expand_dims(instance, axis=2)\n",
    "    expanded = np.tile(expanded, (1, 1, 3))  # overwrite instance by tiled one\n",
    "    \n",
    "    edgecosts = np.asarray(edgecosts)\n",
    "    env_costs = edgecosts[:,1] # np.sum(edgecosts, axis=1) # \n",
    "    normed_env_costs = (env_costs-np.min(env_costs))/(np.max(env_costs)-np.min(env_costs))\n",
    "    # colour nodes in path in red\n",
    "    for i, (x, y) in enumerate(path):\n",
    "        # print(edgecosts[i])\n",
    "        val = edgecosts[i,0]\n",
    "        expanded[x - buffer:x + buffer + 1,\n",
    "                 y - buffer:y + buffer + 1] = [0.9, 1-normed_env_costs[i], 0.2]  # colour red\n",
    "\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    plt.imshow(np.swapaxes(expanded, 1,0), origin=\"upper\")\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: plot on instance\n",
    "plot_path_costs(instance, scaled_path, edgecosts, out_path = \"angle_costs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../outputs/path_37054_directed_startend_infos.json\"\n",
    "with open(os.path.join(json_file_path),\"r\") as infile:\n",
    "    infos = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(infos[\"time_logs\"][\"edge_list_times\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is adding edges faster when nodes are first added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = g.add_vertex(200001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [[i,i+1] for i in range(200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "g.add_edge_list(edges)\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Piecewise linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwlf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = p_arr[:,0]\n",
    "y = p_arr[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pwlf = pwlf.PiecewiseLinFit(x,y)\n",
    "breaks = my_pwlf.fit(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = np.linspace(x.min(), x.max(), 100)\n",
    "y_hat = my_pwlf.predict(x_hat)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x_hat, y_hat, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files = \"tifs_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = pd.read_csv(os.path.join(path_files,\"layer_weights.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tif(path):\n",
    "    with rasterio.open(path, 'r') as ds:\n",
    "        print(path)\n",
    "        arr = ds.read()\n",
    "    return arr[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forbidden ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_constraints(path_files, weights):\n",
    "    hard_cons_rows = weights[weights[\"weight\"]==\"Forbidden\"]\n",
    "    hard_constraints = np.asarray([read_tif(os.path.join(path_files, \"tif_layers\", fname+\".tif\")) for fname in hard_cons_rows[\"Layer Name\"]])\n",
    "    # set to zero\n",
    "    for hard in hard_constraints:\n",
    "        plt.imshow(hard)\n",
    "        plt.show()\n",
    "    hard_constraints -= np.min(hard_constraints)\n",
    "    hard_constraints = np.all(hard_constraints.astype(int), axis=0)\n",
    "    return hard_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hard_constraints = get_hard_constraints(path_files, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = weights[weights[\"weight\"]!=\"Forbidden\"] # weights_wo_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERAL_SHAPE = (1313, 1511)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cost_sum_arr = np.zeros(GENERAL_SHAPE)\n",
    "for fname, weight in zip(layers[\"Layer Name\"], layers[\"weight\"]):\n",
    "    file_path = os.path.join(path_files, \"tif_layers\", fname+\".tif\")\n",
    "    if os.path.exists(file_path):\n",
    "        costs = read_tif(file_path)\n",
    "    costs = np.absolute(normalize(costs)-1)\n",
    "    print(np.min(costs), np.max(costs))\n",
    "    cost_sum_arr = cost_sum_arr + costs * int(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(normalize(cost_sum_arr))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_files = \"tif_ras_buf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(os.path.join(path_files, \"corridor/CORRIDOR_BE.tif\")) as dataset:\n",
    "    print(dataset.width)\n",
    "    print(dataset.bounds)\n",
    "    transform_matrix = dataset.transform\n",
    "    arr = dataset.read()\n",
    "    print(arr.shape)\n",
    "    print(dataset.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:5, 1] = 1\n",
    "a[4, 1:8]=1\n",
    "a[4, 10]=1\n",
    "a[7:15, 10] = 1\n",
    "a[14, 10] = 1\n",
    "a[14, 13:15] = 1\n",
    "a[14:, 14] = 1\n",
    "\n",
    "start = [0,1]\n",
    "end = [19, 14]\n",
    "a[start[0], start[1]] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = read_tif(os.path.join(path_files, \"corridor/CORRIDOR_BE.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corridor = arr2!=9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together = corr.astype(int)*hard_constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "together.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.pad(together, ((1,1), (1,1))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.zeros(together.shape)\n",
    "corr[40:1260, 200:1000] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inds, y_inds = np.where(corr)\n",
    "x_len, y_len = corr.shape\n",
    "min_dist = min([x_inds[0], x_len-x_inds[-1], y_inds[0], y_len-y_inds[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(hard_constraints)# [600:800, 300:720]) # np.swapaxes(arr[0], 0,1)) # , origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "together[650:]\n",
    "plt.imshow(together[650:680, 690:720]) # np.swapaxes(arr[0], 0,1)) # , origin=\"lower\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile\n",
    "sf = shapefile.Reader(\"tifs_new/Destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = sf.shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes[0].points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../outputs/path_37054_directed_startend_infos.json\", \"r\") as infile:\n",
    "    coords = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj, transform\n",
    "inProj = Proj(init=dataset.crs)\n",
    "outProj = Proj(init='epsg:4326')\n",
    "x1,y1 =126131.1214, 178356.2068\n",
    "x2,y2 = transform(inProj,outProj,x1,y1)\n",
    "transformed = []\n",
    "for point in coords[\"path_cells\"]:\n",
    "    trans = transform_matrix * point\n",
    "    x2,y2 = transform(inProj,outProj,trans[0],trans[1])\n",
    "    transformed.append([y2,x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"0\": \"Lat\", \"1\": \"Long\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to correct coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords_arr = np.asarray(coords[\"path_cells\"])\n",
    "switched = np.roll(coords_arr, 1, axis=1)\n",
    "coords_arr = np.asarray([list(transform_matrix*switched[i]) for i in range(len(switched))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(coords_arr, columns=[\"X\", \"Y\"])\n",
    "# df.to_csv(\"converted_coords.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from power_planner.utils.utils import get_half_donut\n",
    "\n",
    "\n",
    "def bresenham_line(x0, y0, x1, y1):\n",
    "    \"\"\"\n",
    "    find pixels on line between two pixels\n",
    "    https://stackoverflow.com/questions/50995499/generating-pixel-values-of-line-connecting-2-points\n",
    "    \"\"\"\n",
    "    steep = abs(y1 - y0) > abs(x1 - x0)\n",
    "    if steep:\n",
    "        x0, y0 = y0, x0\n",
    "        x1, y1 = y1, x1\n",
    "\n",
    "    switched = False\n",
    "    if x0 > x1:\n",
    "        switched = True\n",
    "        x0, x1 = x1, x0\n",
    "        y0, y1 = y1, y0\n",
    "\n",
    "    if y0 < y1:\n",
    "        ystep = 1\n",
    "    else:\n",
    "        ystep = -1\n",
    "\n",
    "    deltax = x1 - x0\n",
    "    deltay = abs(y1 - y0)\n",
    "    error = -deltax / 2\n",
    "    y = y0\n",
    "\n",
    "    line = []\n",
    "    for x in range(x0, x1 + 1):\n",
    "        if steep:\n",
    "            line.append([y, x])\n",
    "        else:\n",
    "            line.append([x, y])\n",
    "\n",
    "        error = error + deltay\n",
    "        if error > 0:\n",
    "            y = y + ystep\n",
    "            error = error - deltax\n",
    "    if switched:\n",
    "        line.reverse()\n",
    "    return line\n",
    "\n",
    "\n",
    "def get_kernel(shifts):\n",
    "    \"\"\"\n",
    "    Get all kernels describing the path of the edges in a discrete raster\n",
    "    :param shifts: possible circle points\n",
    "    :returns kernel: all possible kernels (number of circle points x upper x upper)\n",
    "    :returns posneg: a list indicating whether it is a path to the left (=1) or to the right(=0)\n",
    "    \"\"\"\n",
    "    upper = np.amax(np.absolute(shifts)) + 1\n",
    "    posneg = []\n",
    "    kernel = np.zeros((len(shifts), upper, upper))\n",
    "\n",
    "    for i, shift in enumerate(shifts):\n",
    "        if shift[1] < 0:\n",
    "            posneg.append(1)\n",
    "            line = bresenham_line(0, upper - 1, shift[0], upper - 1 + shift[1])\n",
    "        else:\n",
    "            posneg.append(0)\n",
    "            line = bresenham_line(0, 0, shift[0], shift[1])\n",
    "        # add points of line to the kernel\n",
    "        for (j, k) in line:\n",
    "            kernel[i, j, k] += 1\n",
    "    return kernel, posneg\n",
    "\n",
    "\n",
    "def convolve(img, kernel, neg=0):\n",
    "    \"\"\"\n",
    "    Convolve a 2d img with a kernel, storing the output in the cell \n",
    "    corresponding the the left or right upper corner \n",
    "    :param img: 2d numpy array\n",
    "    :param kernel: kernel (must have equal size and width)\n",
    "    :param neg: if neg=0, store in upper left corner, if neg=1, store in upper \n",
    "    right corner\n",
    "    :return convolved image of same size\n",
    "    \"\"\"\n",
    "    k_size = len(kernel)\n",
    "    if neg:\n",
    "        padded = np.pad(img, ((0, k_size - 1), (k_size - 1, 0)))\n",
    "    else:\n",
    "        padded = np.pad(img, ((0, k_size), (0, k_size)))\n",
    "    # print(padded.shape)\n",
    "    convolved = np.zeros(img.shape)\n",
    "    w, h = img.shape\n",
    "    for i in range(0, w):\n",
    "        for j in range(0, h):\n",
    "            patch = padded[i:i + k_size, j:j + k_size]\n",
    "            convolved[i, j] = np.sum(patch * kernel)\n",
    "    return convolved\n",
    "\n",
    "\n",
    "def angle(path):\n",
    "    path = np.asarray(path)\n",
    "    for p, (i, j) in enumerate(path[:-2]):\n",
    "        v1 = path[p + 1] - path[p]\n",
    "        v2 = path[p + 1] - path[p + 2]\n",
    "        v1_norm = np.linalg.norm(v1)\n",
    "        v2_norm = np.linalg.norm(v2)\n",
    "        angle = np.arccos(np.dot(v1, v2))\n",
    "        if angle < np.pi:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Questions:\n",
    "## altitude leads to more costs for pylons? because they are higher?\n",
    "\n",
    "# height profile constraints:\n",
    "## simply exclude edges which cannot be placed --> only works when iterating over edges\n",
    "\n",
    "## Angle constraints:\n",
    "# * line graph\n",
    "# * path straighening toolbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(0,81,1).reshape((9,9))\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts = get_half_donut(3, 8)\n",
    "kernel, posneg = get_kernel(shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolve(arr, kernel[14], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_faster(img, f, neg):\n",
    "    k_size = len(f)\n",
    "    # a = np.pad(img, ((0, k_size-1), (0, k_size-1)))\n",
    "    if neg:\n",
    "        padded = np.pad(img, ((0, k_size - 1), (k_size - 1, 0)))\n",
    "    else:\n",
    "        padded = np.pad(img, ((0, k_size - 1), (0, k_size - 1)))\n",
    "    \n",
    "    s = f.shape + tuple(np.subtract(padded.shape, f.shape) + 1)\n",
    "    strd = np.lib.stride_tricks.as_strided\n",
    "    subM = strd(padded, shape = s, strides = padded.strides * 2)\n",
    "    return np.einsum('ij,ijkl->kl', f, subM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2 = convolve_faster(arr, kernel[14], 1) # [5:-2, 2:-5] # 8x8\n",
    "# [3:-6, 9:] # for 10x10 kernel, 1:-3,3:-1] # for 5x5 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_faster(img, kernel, neg):\n",
    "    k_size = len(kernel)\n",
    "    print(k_size)\n",
    "    # print(k_size)\n",
    "    # if neg:\n",
    "    #     padded = np.pad(img, ((0, k_size - 1), (k_size - 1, 0)))\n",
    "    # else:\n",
    "    #     padded = np.pad(img, ((0, k_size), (0, k_size)))\n",
    "    padded = np.pad(img, ((0, k_size), (0, k_size)))\n",
    "    print(padded.astype(int))\n",
    "    print(kernel)\n",
    "    return convolve2d(padded, kernel, mode =\"same\")[:-k_size-4, :-k_size-4]\n",
    "# 8x8: [1:-k_size+1, 1:-k_size+1] \n",
    "# 7x7 [2:-k_size+2, :-k_size]\n",
    "# 6x6 [3:-k_size+3, 1:-k_size+1]\n",
    "# 5x5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel[33].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
